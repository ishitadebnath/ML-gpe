{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phenotype: Days to Flowering - Detailed Code.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishitadebnath/ML-gpe/blob/master/Phenotype_Days_to_Flowering_Detailed_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C86aZqbVWyUt"
      },
      "source": [
        "# **Phenotype Prediction: Days to Flowering**\n",
        "\n",
        "The phenotype of an organism is the result of an interplay betweeen its genetic composition and the environment. Due to the gradual change of the climate, it becomes essential to understand the influence of genotype and the environment on plant phenotypes. Here, we intend to develop a machine learning (ML) model, which could predict the phenotype: Days to Flowering. Currently, our model can predict the phenotypic attributes in Sorghum bicolor and we intend to solve this problem in other plant species as well. The model takes the genetic data of the plants and environmental data as input. The gene data is based on Single Nucleotide Polymorphisms (SNP). It is filtered by entropy and added as features in the model. The environmental data consists of location of the plant (range and column of the subplot) and weather parameters from the weather station. Also, sowing date of the plants and window size are required to give as input parameter to the model. \n",
        "\n",
        "Here, we compare verious machine learning models on these dataset with 5-fold cross-validation in order select the final model. The models were evaluated based on Root Mean Squared Error (RMSE) and we find Extreme Gradient Boosting (XgBoost) regressor to perform best on these dataset. Therefore, finally the script outputs a trained model with XgBoost regressor, which can be used to predict the phenotype.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxmsyXVNW7i0"
      },
      "source": [
        "**Dataset:**\n",
        "The following dataset is required in a particular format.\n",
        "\n",
        "**1. Trait Data of Plant Species:** Each observation in this dataset denotes the location (range and column) of the subplot, cultivar information, the date and days of flowering.\n",
        "\n",
        "*Columns: plot, range, column, scientificname, genotype, treatment, blocking_height, method, date_of_flowering, days_to_flowering, gdd_to_flowering, method_type*\n",
        "\n",
        "**2. Gene Data Filtered by Entropy:** The gene data comprises of SNPS from 4.4k genes present in 362 cultivars. The gene data is clustered using k-means with Euclidean distances for 30 gene clusters. Thus, the number of SNPs that each cultivar has in 30 gene clusters is added as features in the model.\n",
        "\n",
        "**3. Weather Data:** The weather data is obtained from a local weather station and the parameters include temperature, relative humidity and vapor pressure deficit.\n",
        "\n",
        "*Columns: date, day_of_year, temp_min, temp_max, temp_mean, gdd, rh_min, rh_max, rh_mean, vpd_mean, precip, precip_cumulative, first_water_deficit_treatment, second_water_deficit_treatment*\n",
        "\n",
        "**4. Sowing Date:** The date at which the seeds of the plants were sowed.\n",
        "\n",
        "**5. Window Size:** The number of days from the sowing date for which the environmental variables are required to be used to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fCn1Ls3XnZ2"
      },
      "source": [
        "The following script contains all the steps, that we followed to build the model for Sorghum Bicolor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MVzsbKE4buc"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, KFold, GroupKFold, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import LassoLars\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import xgboost as xgb\n",
        "import statistics\n",
        "import os\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wds2dWQqtvF",
        "outputId": "95079749-c10a-404e-b64a-8b411578c068"
      },
      "source": [
        "# This code block is not required to be executed if any github URL or a path on local machine is being provides as the path for the data. \n",
        "# Since, currently we are using Google Colab notebook and data is store on Google Drive, hence the drive location is mounted over here. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EugZrYDr_4ft"
      },
      "source": [
        "mac=pd.read_csv(\"/content/drive/MyDrive/GPE/MAC Season 4/mac_season_4_days_gdd_to_flowering.csv\" , delimiter=\",\")\n",
        "mac=mac.rename(columns={\"genotype\": \"cultivar\"})\n",
        "fl2=mac[[\"cultivar\", \"range\", \"column\", \"date_of_flowering\", \"days_to_flowering\"]]\n",
        "\n",
        "ksu=pd.read_csv(\"/content/drive/MyDrive/GPE/KSU/ksu_days_gdd_to_flowering.csv\" , delimiter=\",\")\n",
        "ksu=ksu.rename(columns={\"pass\": \"column\", \"value\": \"days_to_flowering\"})\n",
        "fl1=ksu[[\"cultivar\", \"range\", \"column\", \"date_of_flowering\", \"days_to_flowering\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cblulw59_9Wx"
      },
      "source": [
        "dist=pd.read_csv(\"/content/drive/MyDrive/GPE/Cluster.txt\", delimiter=\"\\t\")\n",
        "a=dist.X\n",
        "dist=dist.rename(columns={\"X\": \"cultivar\"})\n",
        "dist=dist.dropna()\n",
        "\n",
        "fl1=pd.merge(fl1, dist, on=\"cultivar\", how='inner')\n",
        "\n",
        "fl2=pd.merge(fl2, dist, on=\"cultivar\", how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2WMmf1RARh7"
      },
      "source": [
        "ksw=pd.read_csv(\"/content/drive/MyDrive/GPE/KSU/ksu_weather.csv\" , delimiter=\",\")\n",
        "ksw['date']=pd.to_datetime(ksw.date)\n",
        "day_0=pd.to_datetime('2016-06-17')\n",
        "day_n=day_0 + pd.to_timedelta(20, 'days')\n",
        "\n",
        "#for mean\n",
        "tmax = []\n",
        "tmean=[]\n",
        "tmin = []\n",
        "rhmax=[]\n",
        "rhmin=[]\n",
        "rhmean=[]\n",
        "vpd=[]\n",
        " \n",
        "for j in range(fl1.shape[0]):\n",
        "    li = (ksw['date']>= day_0) & (ksw['date'] <= day_n)\n",
        "    df_1 = ksw[li]\n",
        "    tmax.append(df_1['temp_max'].mean())\n",
        "    tmin.append(df_1['temp_min'].mean())\n",
        "    tmean.append(df_1['temp_mean'].mean())\n",
        "    rhmax.append(df_1['rh_max'].mean())\n",
        "    rhmin.append(df_1['rh_min'].mean())\n",
        "    rhmean.append(df_1['rh_mean'].mean())\n",
        "    vpd.append(df_1['vpd_mean'].mean())\n",
        "\n",
        "fl1.insert(1,'temp_max_mean',tmax)\n",
        "fl1.insert(2,'temp_min_mean',tmin)\n",
        "fl1.insert(3,'temp_mean_mean',tmean)\n",
        "fl1.insert(4,'rh_max_mean',rhmax)\n",
        "fl1.insert(5,'rh_min_mean',rhmin)\n",
        "fl1.insert(6,'rh_mean_mean',rhmean)\n",
        "fl1.insert(7,'vpd_mean',vpd)\n",
        "\n",
        "#for maximum\n",
        "tmax = []\n",
        "tmean=[]\n",
        "tmin = []\n",
        "rhmax=[]\n",
        "rhmin=[]\n",
        "rhmean=[]\n",
        "vpd=[]\n",
        "\n",
        "\n",
        "for j in range(fl1.shape[0]):\n",
        "    li = (ksw['date']>= day_0) & (ksw['date'] <= day_n)\n",
        "    df_1 = ksw[li]\n",
        "    tmax.append(df_1['temp_max'].max())\n",
        "    tmin.append(df_1['temp_min'].max())\n",
        "    tmean.append(df_1['temp_mean'].max())\n",
        "    rhmax.append(df_1['rh_max'].max())\n",
        "    rhmin.append(df_1['rh_min'].max())\n",
        "    rhmean.append(df_1['rh_mean'].max())\n",
        "    vpd.append(df_1['vpd_mean'].max())\n",
        "\n",
        "\n",
        "fl1.insert(1,'temp_max_max',tmax)\n",
        "fl1.insert(2,'temp_min_max',tmin)\n",
        "fl1.insert(3,'temp_mean_max',tmean)\n",
        "fl1.insert(4,'rh_max_max',rhmax)\n",
        "fl1.insert(5,'rh_min_max',rhmin)\n",
        "fl1.insert(6,'rh_mean_max',rhmean)\n",
        "fl1.insert(7,'vpd_max',vpd)\n",
        "\n",
        "#for minimum\n",
        "tmax = []\n",
        "tmean=[]\n",
        "tmin = []\n",
        "rhmax=[]\n",
        "rhmin=[]\n",
        "rhmean=[]\n",
        "vpd=[]\n",
        "\n",
        "for j in range(fl1.shape[0]):\n",
        "    li = (ksw['date']>= day_0) & (ksw['date'] <= day_n)\n",
        "    df_1 = ksw[li]\n",
        "    tmax.append(df_1['temp_max'].min())\n",
        "    tmin.append(df_1['temp_min'].min())\n",
        "    tmean.append(df_1['temp_mean'].min())\n",
        "    rhmax.append(df_1['rh_max'].min())\n",
        "    rhmin.append(df_1['rh_min'].min())\n",
        "    rhmean.append(df_1['rh_mean'].min())\n",
        "    vpd.append(df_1['vpd_mean'].min())\n",
        "\n",
        "fl1.insert(1,'temp_max_min',tmax)\n",
        "fl1.insert(2,'temp_min_min',tmin)\n",
        "fl1.insert(3,'temp_mean_min',tmean)\n",
        "fl1.insert(4,'rh_max_min',rhmax)\n",
        "fl1.insert(5,'rh_min_min',rhmin)\n",
        "fl1.insert(6,'rh_mean_min',rhmean)\n",
        "fl1.insert(7,'vpd_min',vpd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq4P-I1BA48L"
      },
      "source": [
        "mac=pd.read_csv(\"/content/drive/MyDrive/GPE/weather_data/mac_season_4_weather.csv\" , delimiter=\",\")\n",
        "mac['date']=pd.to_datetime(mac.date)\n",
        "day_0=pd.to_datetime('2017-04-20')\n",
        "day_n=day_0 + pd.to_timedelta(20, 'days')\n",
        "\n",
        "#for mean\n",
        "tmax = []\n",
        "tmean=[]\n",
        "tmin = []\n",
        "rhmax=[]\n",
        "rhmin=[]\n",
        "rhmean=[]\n",
        "vpd=[]\n",
        " \n",
        "for j in range(fl2.shape[0]):\n",
        "    li = (mac['date']>= day_0) & (mac['date'] <= day_n)\n",
        "    df_1 = mac[li]\n",
        "    tmax.append(df_1['temp_max'].mean())\n",
        "    tmin.append(df_1['temp_min'].mean())\n",
        "    tmean.append(df_1['temp_mean'].mean())\n",
        "    rhmax.append(df_1['rh_max'].mean())\n",
        "    rhmin.append(df_1['rh_min'].mean())\n",
        "    rhmean.append(df_1['rh_mean'].mean())\n",
        "    vpd.append(df_1['vpd_mean'].mean())\n",
        "\n",
        "fl2.insert(1,'temp_max_mean',tmax)\n",
        "fl2.insert(2,'temp_min_mean',tmin)\n",
        "fl2.insert(3,'temp_mean_mean',tmean)\n",
        "fl2.insert(4,'rh_max_mean',rhmax)\n",
        "fl2.insert(5,'rh_min_mean',rhmin)\n",
        "fl2.insert(6,'rh_mean_mean',rhmean)\n",
        "fl2.insert(7,'vpd_mean',vpd)\n",
        "\n",
        "#for maximum\n",
        "tmax = []\n",
        "tmean=[]\n",
        "tmin = []\n",
        "rhmax=[]\n",
        "rhmin=[]\n",
        "rhmean=[]\n",
        "vpd=[]\n",
        "\n",
        "\n",
        "for j in range(fl2.shape[0]):\n",
        "    li = (mac['date']>= day_0) & (mac['date'] <= day_n)\n",
        "    df_1 = mac[li]\n",
        "    tmax.append(df_1['temp_max'].max())\n",
        "    tmin.append(df_1['temp_min'].max())\n",
        "    tmean.append(df_1['temp_mean'].max())\n",
        "    rhmax.append(df_1['rh_max'].max())\n",
        "    rhmin.append(df_1['rh_min'].max())\n",
        "    rhmean.append(df_1['rh_mean'].max())\n",
        "    vpd.append(df_1['vpd_mean'].max())\n",
        "\n",
        "\n",
        "fl2.insert(1,'temp_max_max',tmax)\n",
        "fl2.insert(2,'temp_min_max',tmin)\n",
        "fl2.insert(3,'temp_mean_max',tmean)\n",
        "fl2.insert(4,'rh_max_max',rhmax)\n",
        "fl2.insert(5,'rh_min_max',rhmin)\n",
        "fl2.insert(6,'rh_mean_max',rhmean)\n",
        "fl2.insert(7,'vpd_max',vpd)\n",
        "\n",
        "#for minimum\n",
        "tmax = []\n",
        "tmean=[]\n",
        "tmin = []\n",
        "rhmax=[]\n",
        "rhmin=[]\n",
        "rhmean=[]\n",
        "vpd=[]\n",
        "\n",
        "for j in range(fl2.shape[0]):\n",
        "    li = (mac['date']>= day_0) & (mac['date'] <= day_n)\n",
        "    df_1 = mac[li]\n",
        "    tmax.append(df_1['temp_max'].min())\n",
        "    tmin.append(df_1['temp_min'].min())\n",
        "    tmean.append(df_1['temp_mean'].min())\n",
        "    rhmax.append(df_1['rh_max'].min())\n",
        "    rhmin.append(df_1['rh_min'].min())\n",
        "    rhmean.append(df_1['rh_mean'].min())\n",
        "    vpd.append(df_1['vpd_mean'].min())\n",
        "\n",
        "fl2.insert(1,'temp_max_min',tmax)\n",
        "fl2.insert(2,'temp_min_min',tmin)\n",
        "fl2.insert(3,'temp_mean_min',tmean)\n",
        "fl2.insert(4,'rh_max_min',rhmax)\n",
        "fl2.insert(5,'rh_min_min',rhmin)\n",
        "fl2.insert(6,'rh_mean_min',rhmean)\n",
        "fl2.insert(7,'vpd_min',vpd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LvA6iWlCXMP"
      },
      "source": [
        "data=fl2.append(fl1)\n",
        "new=data.drop(['cultivar', 'date_of_flowering', 'days_to_flowering'], axis=1)\n",
        "X=new.to_numpy()\n",
        "min_max_scaler = MinMaxScaler()\n",
        "X = min_max_scaler.fit_transform(X)\n",
        "\n",
        "y=data[['days_to_flowering']]\n",
        "y = np.asarray(y).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzYHiBsfF6jZ",
        "outputId": "3dc6bfc1-02dc-41f9-f628-e7de66cce948"
      },
      "source": [
        "#Hyperparameter tuning for the models\n",
        "model = RandomForestRegressor()\n",
        "max_depth = range(1, 20, 2)\n",
        "n_estimators=range(20, 55, 5)\n",
        "groups=data.cultivar\n",
        "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
        "kfold = GroupKFold(n_splits=5)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, y, groups)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), \"for RF\")\n",
        "\n",
        "model = xgb.XGBRegressor()\n",
        "n_estimators = range(10, 60, 5)\n",
        "learning_rate = np.linspace(0,1,11)\n",
        "max_depth = range(1, 20, 2)\n",
        "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate)\n",
        "kfold = GroupKFold(n_splits=5)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, y, groups)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), \"for XgB\")\n",
        "\n",
        "model = SGDRegressor()\n",
        "alpha = np.arange(0.01, 1.0, 0.05)\n",
        "param_grid = dict(alpha=alpha)\n",
        "kfold = GroupKFold(n_splits=5)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, y, groups)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), \"for SGD\")\n",
        "\n",
        "model = LassoLars()\n",
        "alpha = np.arange(0.1, 1.0, 0.01)\n",
        "param_grid = dict(alpha=alpha)\n",
        "kfold = GroupKFold(n_splits=5)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, y, groups)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), \"for LL\")\n",
        "\n",
        "model = Lasso()\n",
        "alpha = np.arange(0.1, 1.0, 0.01)\n",
        "\n",
        "param_grid = dict(alpha=alpha)\n",
        "kfold = GroupKFold(n_splits=5)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, y, groups)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), \"for L\")\n",
        "model = Ridge()\n",
        "alpha = np.arange(0.1, 1.0, 0.01)\n",
        "param_grid = dict(alpha=alpha)\n",
        "kfold = GroupKFold(n_splits=5)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, y, groups)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), \"for R\")\n",
        "\n",
        "model = ElasticNet()\n",
        "alpha = np.arange(0.01, 1.0, 0.05)\n",
        "l1_ratio=np.arange(0, 1, 0.01)\n",
        "param_grid = dict(alpha=alpha, l1_ratio=l1_ratio)\n",
        "kfold = GroupKFold(n_splits=5)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, y, groups)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), \"for EN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: -11.358709 using {'max_depth': 9, 'n_estimators': 20} for RF\n",
            "[17:13:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Best: -11.370335 using {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 55} for XgB\n",
            "Best: -13.803746 using {'alpha': 0.11} for SGD\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.748e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.135e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.135e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: -13.724154 using {'alpha': 0.1} for LL\n",
            "Best: -13.164801 using {'alpha': 0.1} for L\n",
            "Best: -13.081240 using {'alpha': 0.9899999999999995} for R\n",
            "Best: -13.038583 using {'alpha': 0.060000000000000005, 'l1_ratio': 0.99} for EN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez9fVYwB0I5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fad223-441e-4ffc-906b-cc1c3e5c7756"
      },
      "source": [
        "#Implementation of different algorithms\n",
        "\n",
        "# 5 fold cross-validation for Random Forest\n",
        "groups=data.cultivar\n",
        "ns=5\n",
        "cv = GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X, y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    print('TRAIN :',X_train.shape, 'TEST :', X_test.shape)\n",
        "    crf=RandomForestRegressor(max_depth=9, n_estimators=20).fit(X_train, y_train)\n",
        "\n",
        "    pred=crf.predict(X_test)\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "std= (statistics.stdev(e))\n",
        "print('Root Mean square error for 5-fold CV RF',error/ns,'+-', std)\n",
        "\n",
        "# 5 fold cross-validation for Stochastic Gradient Descent Regression\n",
        "\n",
        "cv =GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "R2=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X,y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    clm=SGDRegressor(alpha=0.11).fit(X_train, y_train)\n",
        "    pred=clm.predict(X_test)\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    \n",
        "std= (statistics.stdev(e))\n",
        "\n",
        "print('Root Mean square error for 5-fold CV SG:',error/ns, '+-', std)\n",
        "\n",
        "# 5 fold cross-validation for Linear Regression\n",
        "cv =GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "R2=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X,y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    clm=LinearRegression().fit(X_train, y_train)\n",
        "    pred=clm.predict(X_test)\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    \n",
        "std= (statistics.stdev(e))\n",
        "\n",
        "print('Root Mean square error for 5-fold CV LR:',error/ns, '+-', std)\n",
        "\n",
        "# 5 fold cross-validation for XgBoost Regression\n",
        "cv =GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "R2=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X,y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    clm=xgb.XGBRegressor(booster='gbtree',importance_type='gain',learning_rate= 0.1, max_depth= 5, n_estimators= 55).fit(X_train, y_train)\n",
        "    pred=clm.predict(X_test)\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    \n",
        "std= (statistics.stdev(e))\n",
        "\n",
        "print('Root Mean square error for 5-fold CV XG:',error/ns, '+-', std)\n",
        "\n",
        "# 5 fold cross-validation for LassoLars Regression\n",
        "cv =GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "R2=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X,y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    clm=LassoLars(alpha=0.1).fit(X_train, y_train)\n",
        "    pred=clm.predict(X_test)\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    \n",
        "std= (statistics.stdev(e))\n",
        "\n",
        "print('Root Mean square error for 5-fold CV LL:',error/ns, '+-', std)\n",
        "\n",
        "# 5 fold cross-validation for Lasso Regression\n",
        "cv =GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "R2=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X,y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    clm=Lasso(alpha=0.1).fit(X_train, y_train)\n",
        "    pred=clm.predict(X_test)\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    \n",
        "std= (statistics.stdev(e))\n",
        "\n",
        "print('Root Mean square error for 5-fold CV L:',error/ns, '+-', std)\n",
        "\n",
        "# 5 fold cross-validation for Ridge Regression\n",
        "cv =GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "R2=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X,y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    clm=Ridge(alpha=0.99).fit(X_train, y_train)\n",
        "    pred=clm.predict(X_test)\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    \n",
        "std= (statistics.stdev(e))\n",
        "\n",
        "print('Root Mean square error for 5-fold CV R:',error/ns, '+-', std)\n",
        "\n",
        "# 5 fold cross-validation for ElasticNet Regression\n",
        "cv =GroupKFold(n_splits=ns)\n",
        "error=0\n",
        "R2=0\n",
        "e=[]\n",
        "for train_index, test_index in cv.split(X,y,groups):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    clm=ElasticNet(alpha= 0.06, l1_ratio= 0.99, max_iter=5000).fit(X_train, y_train)\n",
        "    pred=clm.predict(X_test)\n",
        "    error=error+math.sqrt(mean_squared_error(y_test, pred))\n",
        "    e.append(math.sqrt(mean_squared_error(y_test, pred)))\n",
        "    \n",
        "std= (statistics.stdev(e))\n",
        "\n",
        "print('Root Mean square error for 5-fold CV EN:',error/ns, '+-', std)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN : (189, 53) TEST : (48, 53)\n",
            "TRAIN : (189, 53) TEST : (48, 53)\n",
            "TRAIN : (190, 53) TEST : (47, 53)\n",
            "TRAIN : (190, 53) TEST : (47, 53)\n",
            "TRAIN : (190, 53) TEST : (47, 53)\n",
            "Root Mean square error for 5-fold CV RF 12.064083670847563 +- 1.8444612525272905\n",
            "Root Mean square error for 5-fold CV SG: 13.875197365359536 +- 0.9951118205406825\n",
            "Root Mean square error for 5-fold CV LR: 15.57581548315363 +- 1.5015688849074071\n",
            "[17:35:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[17:35:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[17:35:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[17:35:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[17:35:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Root Mean square error for 5-fold CV XG: 11.37033494574618 +- 2.1766643136057238\n",
            "Root Mean square error for 5-fold CV LL: 13.724153798445059 +- 1.1203724203115142\n",
            "Root Mean square error for 5-fold CV L: 13.164801474850488 +- 0.9893025011906886\n",
            "Root Mean square error for 5-fold CV R: 13.081240091689647 +- 0.8339155000191717\n",
            "Root Mean square error for 5-fold CV EN: 13.038583078270317 +- 0.8785822942026872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.605e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.004e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.099e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.099e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.099e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.218e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.309e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.370e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.155e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.155e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:582: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.272e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}